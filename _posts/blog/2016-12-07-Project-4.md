---
layout: post
title: "Build the (Sea) Wall? Predicting Tide Height"
modified:
categories: blog
excerpt:
tags: []
image:
  feature:
date: 2016-12-07T08:08:50-04:00
---

# Markov Chains in a Class by Themselves

We have started moving into the more heavy duty analyses, starting off with a big one - Bayesian Regressions using the Markov Chain Monte Carlo method emcee. In this project the end goal is to predict if a tide wall is needed on a shoreline by determining the probability of the tide going above 15 feet.

In order to get the result, we had to build a class that performed the emcee operations, as well as the common charting and analyses that go along with the type of regression chosen (in this case, a linear regression).

## Summary
There is no better way to understand how to use an existing method than by moving it into a class. Some quirks of the emcee routine resulted in some of the functions being fed into it not following the standard class nomenclature (namely self. for many of the distributions).

Aside from calling the Markov Chain method from within a class, this project was also instrumental in helping me develop a much deeper understanding of Bayesian Regressions and handling long run times. Unfortunately, putting the emcee method into a class does not lend itself to easy multi-processing.



## Putting The Class Together
For the most part, constructing the class was fairly straightforward, with a little bit of finagling needed with the distributions. Aside from the distributions, the other inputs include:

* Data - the observed data set can be passed in as a list.
* Parameter Names - In the case of this class, var_names. This has a two-fold function - the number of model parameters (needed for emcee) and the names to facilitate plotting with legends.
* Walkers - the number of walkers to be used the the Markov Chain, since this varies from analysis to analysis.
* Regression - in the case of a linear regression, this function is used to plot the regression, it is called when you want to plot the regression not when the class is instantiated.
* Predictive Distribution - varied enough from the regression distribution that I had to have it as a separate argument.

### The Distributions
The emcee method for running the Markov Chains requires the natural log of the posterior distribution density function, with the only variable in the lnprob being a list/tuple of the model parameters. However, the likelihood distribution function structure will vary depending on the data observations, so a two variable distribution is fed into the class along with the data, and then reduced to a single variable likelihood before being combined with the prior.

The other issue with the distributions was that the emcee method did not play well with the typical self. variable labeling inside the class. As a result, the probability distributions were not labeled to be called outside the class, and the full Markov Chain sampling was done inside the class initiation.

## Modeling the Data
With the class all set, it was time to move onto modeling the particular data set provided. The wave model had the shape of a basic sinusoidal function, over a period \\( 2 \pi \\).

\\[ tide = a sin(b(t + c)) + d \\]

* a = amplitude

* b = cycles from 0 to \\( 2 \pi \\)

* c = horizontal shift (sometimes called "phase shift" when B = 1)

* d = vertical shift (or displacement)

In order to be able to plot the regression, this was linearized to approximately:

\\[ tide = a' + (b'sin(t) + c'cos(t)) \\]

### Likelihood (log of atom)

Take the normal distribution with \\( \lambda(t) =\lambda_0 \\) for a fixed and known value \\( \lambda_0 \\) and
<br /><br />
\\[ \mu(t) = \omega_0 + \omega_1 \sin(t) + \omega_2 \cos(t) \\]
<br />
So the resulting wave height equation:

\\[ X_t \sim \sqrt{\frac{\lambda_0}{2 \pi}}e^{\frac{-\lambda_0}{2}(x - (\omega_0 + \omega_1 \sin(t) + \omega_2 \cos(t)))^2} \\]

remove \\( 2\pi \\) as it is not impacted by taking the log, then take the log:

\\[ log(likelihood \  atom) = \frac{1}{2}log(\lambda_0) - \frac{\lambda_0}{2} (x - (\omega_0 + \omega_1 \sin(t) + \omega_2 \cos(t)))^2 \\]


Showing the above in terms of x & e where e is the list of data tuples:

\\[ \frac{1}{2}log(x[3]) - \frac{x[3]}{2} (e[0] - (x[0] + x[1] \sin(t) + x[2] \cos(t)))^2 \\]

### Log Prior

For the Prior \\( \pi \\) we chose the following over \\( \omega_0,\omega_1, \omega_2, \omega_3 \\) *(holding \\( \lambda \\) constant)*:

\\[ \pi(\omega_0,\omega_1,\omega_2, \lambda) = \frac{1}{2 \pi} e^{-\frac{1}{2}\omega_0^2}e^{-\frac{1}{2}\omega_1^2}e^{-\frac{1}{2}\omega_2^2} e^{-\lambda_0} \\]

the log prior is:

\\[ =  {-\frac{1}{2}\omega_0^2} - {\frac{1}{2}\omega_1^2}- {\frac{1}{2}\omega_2^2} - {\lambda_0} \\]


## Moving Towards the Results
\\( \lambda \\) Does throw off the scale a bit, but the other model parameters do come in at a reasonable range given the noise in the data.

<figure class="half">
	<img src="{{site.url}}/images/p4param.png" alt="image">
	<img src="{{site.url}}/images/p4corners.png" alt="image">
	<figcaption>Summary Charts of Model Parameters.</figcaption>
</figure>

Below shows a predictive distribution for a time of t = 10.

<figure>
	<img src="{{site.url}}/images/p4prob_dist.png" alt="image">
	<figcaption>Predictive Distribution for t = 10. </figcaption>
</figure>

And finally, the regression does appear to come in a bit low, but this may be the clustering of data near the end of the period pulling down the data.

<figure>
	<img src="{{site.url}}/images/p4reg.png" alt="image">
	<figcaption>Regression</figcaption>
</figure>

# Build the (Sea) Wall

Using a threshold of a 5% probability, the outcome is to build the sea wall, as indicated a probability of just over 6% that the tide will rise above 15 feet.

# Update
While putting the emcee routine in a class has been useful, one major limitation is that you are not able to run the class on multiple processors as it is does not lend itself easily to pickling. This, combined with the variants needed for linear vs non-linear regression calculations, will result in this idea being put on the shelf until a further need for the class structure warrants the
