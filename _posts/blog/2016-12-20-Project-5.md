---
layout: post
title: "Lending Club Loan Determination"
modified:
categories: blog
excerpt:
tags: []
image:
  feature:
date: 2017-01-24 05:00:00 -0400
---

# More Fun with Bayes

We have recently passed the half-way point in the GA Data Science Immersive program, and the projects just keep getting more interesting. In this project we were asked to build a Bayesian Classification model to predict if a loan for a new customer should be approved given the data available. The data set for [2015](https://www.lendingclub.com/info/download-data.action) from Lending Club was to be used to build the predictor model.

## Summary
The aim of this model is to use a list of factors (income, debt-to-income, loan amount, interest rate, home ownership, and loan term) from the applicant to determine a threshold for accepting the loan, including a justification of the decision criteria.

The model is to be based on 2015 data from Lending Club, again with justification for what determines a good loan (one worth accepting) versus one that should be rejected.

## Method

I have broken this project into three steps (and three Jupyter Notebooks) to facilitate saving progress of various stages without having to rerun the whole notebook each time I start it up. Any data needed between notebooks was saved as a .csv file.

### Part 1: The Data

#### Prepping the data
After loading the data into a dataframe, the columns we want to work with fell into three categories:
1. Good to go - these columns were already in a workable number format.
  * Loan amount
  * Income
  * Debt-to-income
2. Need a little cleaning - This column was a string, with the percentage (%) sign at the end of the number. So the % was stripped, the string converted to a float, and then divided by 100.
  * Interest rate
3. Dividing up categorical data - these columns had a handful of choices and needed to be split up into dummy variables (0 or 1 for each choice).

##### Defining Good vs Bad Loans

Of the various categories in the 'Status' column, good loans were defined as "Fully Paid", with bad loans being tagged with either "Default" or "Charged Off". All the remaining categories showed the loans as still active or in the process of some waiting or grace period where the lenders still had an opportunity to catch up on missing payments.

#### Pulling out only the data needed

In order to make the data easier to work with, the original data has 113 columns and 421,097 rows, I took it through a series of dataframe adjsutments that:
* Removed all loans not closed out (keeping Good & Bad)
* Keeping only the columns needed, excluding one of each of the dummy variable lists for the category columns.
* Renaming some columns to make it more concise.
* [In the end, before moving onto taking Markov Chain samples, I also split it into two dataframes - good and bad loans, to facilitate resampling good loans to better balance classification imbalances - more on this later.]

#### Exploratory Data Analysis

The next step was taking a look at the data to see any trends/relationship that may exist, as well as how the data was shaped.

In doing so, a very small number of outliers were found (see below for debt-to-income)
![debt-to-income]({{ site.baseurl }}/images/p5dti.png "debt-to-income")
For three of the columns, a small number of outliers were removed, out of a total of just over 101,000 data points.


| Variable | Threshold | Data Points dropped |
|-------|-------|:---------:|
|Interest Rate| no change | N/A  |
|Debt to Income| < 50     | 11   |
|Income | <1,000,000 |  31 |
|Loan Amount | no change |  N/A            |
{: .table}

##### Normalizing the data
The four columns listed in the table above were then normalized to provide a greater sensitivity for the model, which will be based on the sigmoid of a linear function.



|Variable     | Method   | Min  | Max   |
|-----|:----:|:----:|:----:|
|Interest Rate| Min-Max | 5.32%      |  28.99%     |
|Debt to Income| Min-Max  |   0.0   |  48.56      |
|Income       | Min-Max |  4,000 | 980,000    |
|Loan Amount  | Min-Max |  1,000      |  35,000    |
{: .table}


#### Classification imbalances

It turns out, the good loans outnumbered the bad loans by just over three to one according to my criteria. In order to resample the data prior to running the MCMC walkers in part 2, I saved the good and bad loans in two separate files. In the 2nd book I then used all the bad loans, and randomly sampled an identical number of good loans to create a new set of equally matched data samples.

### Part 2: Markov Chain Monte Carlo Sampling

This section starts of with creating a balanced set of data samples as described above before going into building the model and running the walkers.

#### The Final Strategy
Aside from circling back to deal with the classification imbalance, I did run through a number of challenges in the modeling which I will detail as needed.

In the two sets of of dummy variables, I removed one of each to reduce the number of variables. Initially I included them all, but since the presence of the others would compensate for the removal of one, and my larger data runs took three hours, I was looking to gain some efficiency.

The likelihood predictive distribution was determined using the sigmoid of a linear relationship between the remaining seven columns, with one additional variable not tied to the data set.

\\[ g(x) =\omega_0 + \omega_1loan\_amt + \omega_2income + \omega_3dti+ \omega_4int\_rate\_p + \omega_5 OWN + \omega_6 MORTGAGE+ \omega_7 36month \\]

\\[ \sigma(x) = \frac{1}{1+e^{-x}} \\]

\\[ p = \sigma(g) = \frac{1}{1+e^{-g}} \\]

So the log atom of the probability density becomes:

\\[ log(\sigma(g)) = -log({1+e^{-g}}) \\]

The prior distribution was a normalized distribution with lambda set to 1.

Using the emcee module, the logs of these distributions were used and combined into the log posterior distribution (with the predictive likelihood being summed across the data set).

The final data set used 10,000 observations (split evenly between good and bad loans), 20 walkers, and 10,000 steps per walker. As described above, this was an eight dimensional model.

In the process of building out this model, and verifying everything was working properly, the number of data sets and steps used varied but was mainly 100, 1,000 and few higher until I settled on the 10,000. The lower numbered runs were for proof of concepts.

<figure>
	<img src="{{site.url}}/images/p5corners.png" alt="image">
	<figcaption>MCMC Corners</figcaption>
</figure>

As a backup sanity check, I also ran a RLS to find the Maximum A Priori to ensure that my MCMC results were ok. These turned out to be fairly close to the max values in the MCMC variable distributions.

Maximum A Priori results:
[ 1.33042487, -0.17342747,  1.92037928, -1.32874063, -3.01579461, 0.12903568,  0.39878407,  0.10152096]

### Part 3: Decision Model

This last part was separated in large part to be able to adjust the prediction model without having to rerun the MCMC sampler (3 hours at the setting used) when adjustments were made.

To facilitate repeated runs, with random data sets each time, I loaded the two data frames of good and bad loans (with their corresponding factors). In addition, the MCMC sample results were also loaded. I then randomly chose 4,000 data points from the loan samples (2,000 good and 2,000 bad) as the set to test the prediction accuracy. The histogram below shows the predicted results.

<figure>
	<img src="{{site.url}}/images/p5sample.png" alt="image">
	<figcaption>Prediction Results</figcaption>
</figure>

#### Testing the Predictions

For each data point, I ran it through the monte carlo integration to see the probability that the loan was good (would be repaid), with the results for all datapoint results being saved in two lists. The lists where then checked for True Positive and False Negative results from the known good loans, and True Negative and False Positive from the know bad loans.

To find the best decision point - in this case defined by highest accuracy with a balance between recall and precision - the results were checked for cutoff points between P = 0.3 and P = 0.7.

<figure>
	<img src="{{site.url}}/images/p5accuracy.png" alt="image">
	<figcaption>Classification Accuracy</figcaption>
</figure>


# Conclusion
Based on the highest accuracy, while balancing precision and recall, the cutoff point for good vs bad loans should be P = 0.549 (peak accuracy). This will give an accuracy of 64.9% based on the sample set of 4,000 closed loans from 2015.

I believe the accuracy of this model was limited in a couple of inter-related aspects.
1. The data used to develop the model is the number of closed loans in 2015. This in itself is inherently biased data in that to grant a loan, Lending Club already has a set of metrics in place (one would hope), which would already weed out (fingers crossed) a high percentage of loans that would go bad.
2. Because a significant set of bad performers have already been removed from the sample pool, you get an oversampling of good performing loans. This can be adjusted for by resampling to match the number of good loans and bad in the data used to fit the model, but it is not a perfect solution.

That said, this model does have a decent accuracy of close to 65%, showing that the criteria that Lending Club already has in place for poor performing loans can be improved.
